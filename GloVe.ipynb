{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe",
      "provenance": [],
      "authorship_tag": "ABX9TyMTp9NDaQcVUeMEXG2f2Tkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrigakshi24-ux/Training-Sheet/blob/main/GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glove-python-binary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BITN_a0uTkWz",
        "outputId": "228a1117-9308-4502-cf7d-491c6557bd03"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: glove-python-binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cy1VthkoalDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fea90cc-307e-4554-d5ba-8ab18e939ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import spatial\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from glove import Corpus, Glove"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to load the pre-trained model hosted at this link, will take a few minutes because large file\n",
        "urllib.request.urlretrieve('https://nlp.stanford.edu/data/glove.6B.zip','glove.6B.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd9jCSuKaqre",
        "outputId": "b3bcbd6b-d18e-4f11-c982-4f6749c83956"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('glove.6B.zip', <http.client.HTTPMessage at 0x7f8419820490>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zip file, unzip it by providing source and destination for the file. Here 50, 100 means dimensions.\n",
        "!unzip \"/content/glove.6B.zip\" -d \"/content/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5-QBhbAa5k6",
        "outputId": "6bf8d5fb-fcb7-4532-ec8d-72473c5566ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/glove.6B.zip\n",
            "replace /content/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/glove.6B.50d.txt  \n",
            "replace /content/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/glove.6B.100d.txt  y\n",
            "\n",
            "replace /content/glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: /content/glove.6B.200d.txt  y\n",
            "\n",
            "replace /content/glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: /content/glove.6B.300d.txt  y\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an embedding dictionary for anyone file from above\n",
        "embed_dict = {}\n",
        "with open('/content/glove.6B.200d.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embedd = np.asarray(values[1:], 'float32')\n",
        "    embed_dict[word] = embedd"
      ],
      "metadata": {
        "id": "DOEdfeMqbKOW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to find similar words, we will use Eucledian distance\n",
        "# sort the keys(words) in ascending order(least distance) with key as eucledian distance btw every word vector and given word vector\n",
        "def find_similar(word_vector, embed = embed_dict):\n",
        "  sim = sorted(embed.keys(), key=lambda word: spatial.distance.euclidean(embed[word], word_vector))\n",
        "  return sim"
      ],
      "metadata": {
        "id": "wIPiKN6xcjvq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def howSimilar(word1, word2, embed = embed_dict):\n",
        "  return (cosine_similarity([list(embed[word1])], [list(embed[word2])]))[0][0]"
      ],
      "metadata": {
        "id": "suYCVD7hvLLC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar(embed_dict['river'])[1:11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ohA3f8Nd-kj",
        "outputId": "a3067742-3e45-4b96-e636-020fe96a35ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rivers',\n",
              " 'tributary',\n",
              " 'confluence',\n",
              " 'creek',\n",
              " 'along',\n",
              " 'tributaries',\n",
              " 'valley',\n",
              " 'flows',\n",
              " 'danube',\n",
              " 'upstream']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "howSimilar('man', 'women')"
      ],
      "metadata": {
        "id": "h8vKZGkeehNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43be731-6e71-40f8-88cb-d94d52378358"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40427884"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "howSimilar('girl', 'women')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UORxMiZWvr8r",
        "outputId": "d79dab27-bc40-496c-ef5c-ae7411bbe0c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47667465"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##On a text"
      ],
      "metadata": {
        "id": "TxU3O4KN337O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines =  [\"Hello this is a tutorial to convert word to integer\",\n",
        "         \"It is a beautiful day\",\n",
        "         \"Jack is going to office\",\n",
        "         \"I want to go Manali, it's very pretty\"]"
      ],
      "metadata": {
        "id": "twKjOeDq1qB_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess\n",
        "\n",
        "# lower\n",
        "doc = list(np.char.lower(lines))\n",
        "\n",
        "# tokenize\n",
        "for i,line in enumerate(doc):\n",
        "  doc[i] = line.split()\n",
        "\n",
        "# # stopwords\n",
        "for j,i in enumerate(doc):\n",
        "  doc[j] = ' '.join(list(filter(lambda x:x not in nltk.corpus.stopwords.words('english'), i)))\n",
        "\n",
        "# # symbols\n",
        "symb = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "for i in symb:\n",
        "  doc = list(np.char.replace(doc, i, ''))\n",
        "doc\n",
        "\n",
        "# # lemmatize\n",
        "wordnet_lemmatizer = WordNetLemmatizer() \n",
        "for j,i in enumerate(doc):\n",
        "  doc[j] = list(filter(lambda x:wordnet_lemmatizer.lemmatize(x, pos ='a'), i.split()))\n",
        "\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOZhm8SI4HiN",
        "outputId": "7f3352a3-5c24-4fd4-e75e-b3abc26522dd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hello', 'tutorial', 'convert', 'word', 'integer'],\n",
              " ['beautiful', 'day'],\n",
              " ['jack', 'going', 'office'],\n",
              " ['want', 'go', 'manali', 'pretty']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a corpus object\n",
        "corpus = Corpus() \n",
        "# training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "corpus.fit(doc, window=10) # changes the user input according to the requirement, window size is the number of surrounding words to be taken, here 10"
      ],
      "metadata": {
        "id": "a06381ZhN1MB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "# We can set the learning rate as it uses Gradient Descent and number of components\n",
        "glove = Glove(no_components=5, learning_rate=0.05) # no_components determines the number of dimensions for output vector"
      ],
      "metadata": {
        "id": "b9ulndqVN7MO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)  # first pattern is the co-occurence matrix\n",
        "\n",
        "# add dictionary to glove object\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9LV4HZfUPI1",
        "outputId": "ad410ce6-6530-437f-9f53-3d5ebaefe0c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 30 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "Epoch 20\n",
            "Epoch 21\n",
            "Epoch 22\n",
            "Epoch 23\n",
            "Epoch 24\n",
            "Epoch 25\n",
            "Epoch 26\n",
            "Epoch 27\n",
            "Epoch 28\n",
            "Epoch 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove.word_vectors[glove.dictionary['manali']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzHvBRnCX0G7",
        "outputId": "bf0f9ba6-67b0-4359-90eb-20ec572443ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.07884263, -0.09927628, -0.02481225, -0.05884735,  0.02997702])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7_LE0cc0ZHXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}